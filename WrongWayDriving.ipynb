{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "c_7MudlXCfu9",
        "gqP_WlTzB6N0",
        "qqzCo_E2IrOs",
        "bI5-kOt6IhR0"
      ],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7W2potWC0/dkqN3+IquV/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnitaTasnim/WrongWayDrivingCode/blob/main/WrongWayDriving.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-QJmFmcymgn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Start"
      ],
      "metadata": {
        "id": "TBn33SRX77-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Usmankhujaev/Wrong-direction-drivers-detection.git\n",
        "%cd Wrong-direction-drivers-detection\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXz2Rxov81ZK",
        "outputId": "605bcebe-fbda-4d5d-d999-118e626b9707"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Wrong-direction-drivers-detection'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 88 (delta 12), reused 4 (delta 4), pack-reused 73 (from 1)\u001b[K\n",
            "Receiving objects: 100% (88/88), 1.53 MiB | 41.14 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n",
            "/content/Wrong-direction-drivers-detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlgXaw6gDBNs",
        "outputId": "29c0f2a6-3d6c-464c-f381-203504763071"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detector_car_person.py\tmodel_data\t    requirenment.yml  train.py\n",
            "KalmanFilter.py\t\tobject_tracking.py  result\t      yolo3\n",
            "kmeans.py\t\tREADME.md\t    tracker.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights -O model_data/yolov3.weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzaksy7YLhoZ",
        "outputId": "e0dd02ce-eb36-42c7-d433-94a40b51cab0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-17 21:15:37--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 172.67.185.199, 104.21.88.156, 2606:4700:3030::ac43:b9c7, ...\n",
            "Connecting to pjreddie.com (pjreddie.com)|172.67.185.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://data.pjreddie.com/files/yolov3.weights [following]\n",
            "--2025-12-17 21:15:37--  https://data.pjreddie.com/files/yolov3.weights\n",
            "Resolving data.pjreddie.com (data.pjreddie.com)... 172.67.185.199, 104.21.88.156, 2606:4700:3030::ac43:b9c7, ...\n",
            "Connecting to data.pjreddie.com (data.pjreddie.com)|172.67.185.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘model_data/yolov3.weights’\n",
            "\n",
            "model_data/yolov3.w 100%[===================>] 236.52M  36.3MB/s    in 5.9s    \n",
            "\n",
            "2025-12-17 21:15:43 (39.8 MB/s) - ‘model_data/yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls model_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUoGtnkMDBPR",
        "outputId": "c001b6be-eaa4-4671-d41b-ea1be93d56f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car_class_2.txt  yolo_anchors.txt  yolov3.weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJzk6UZaDBTd",
        "outputId": "1ffc96e1-035a-4263-abab-b4588cba5e70"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "algorithm2.png\n",
            "result.png\n",
            "wrong_direction_20191118_181432_2_01.jpg\n",
            "wrong_direction_20191118_181432_3_01.jpg\n",
            "wrong_direction_20191118_181433_3_00.jpg\n",
            "wrong_direction_20191118_181433_3_01.jpg\n",
            "wrong_direction_20191118_181501_2_01.jpg\n",
            "wrong_direction_20191118_181502_2_01.jpg\n",
            "wrong_direction_20191118_181502_3_00.jpg\n",
            "wrong_direction_20191118_181503_3_00.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JS6Kw7XDBVt",
        "outputId": "aaaae69a-f5dc-4053-d7af-3f820645f0ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CLEAN ENV FOR COLAB =====\n",
        "'''!pip uninstall -y numpy scipy\n",
        "!pip install --no-cache-dir numpy==1.26.4 scipy==1.11.4\n",
        "!pip install opencv-python filterpy matplotlib pillow ultralytics\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nDeia10Jqbj3",
        "outputId": "e8fe28a0-11e5-420a-abcb-a6f15db11a1d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!pip uninstall -y numpy scipy\\n!pip install --no-cache-dir numpy==1.26.4 scipy==1.11.4\\n!pip install opencv-python filterpy matplotlib pillow ultralytics\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(np.char)\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "print(\"SciPy OK\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEWeD14bvYhQ",
        "outputId": "185b935a-2c69-4faa-a95c-8ed721ed5757"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy: 2.0.2\n",
            "<module 'numpy.char' from '/usr/local/lib/python3.12/dist-packages/numpy/char/__init__.py'>\n",
            "SciPy OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip install -q --force-reinstall \\\n",
        "numpy==1.26.4 \\\n",
        "scipy==1.11.4 \\\n",
        "opencv-python \\\n",
        "opencv-contrib-python \\\n",
        "pillow \\\n",
        "matplotlib \\\n",
        "filterpy \\\n",
        "tensorflow==2.16.1 \\\n",
        "keras\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "e_vyZ2UAGDmh",
        "outputId": "2d76ef80-0f3e-402a-8eb3-a5bcaa7e876c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!pip install -q --force-reinstall numpy==1.26.4 scipy==1.11.4 opencv-python opencv-contrib-python pillow matplotlib filterpy tensorflow==2.16.1 keras\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k6yZqMRIrnNw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#kalman_filter.py"
      ],
      "metadata": {
        "id": "c_7MudlXCfu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "\n",
        "class KalmanFilterClass(object):\n",
        "    def __init__(self):\n",
        "        self.dt = 5e-3\n",
        "        self.A = np.array([[1,0],[0,1]])\n",
        "        self.u = np.zeros((2,1))\n",
        "        self.b = np.array([[0],[255]])\n",
        "        self.P = np.diag((3.0, 3.0))\n",
        "        self.F = np.array([[1.0, self.dt], [0.0, 1.0]])\n",
        "        self.Q = np.eye(self.u.shape[0])\n",
        "        self.R = np.eye(self.b.shape[0])\n",
        "        self.lastResult = np.array([[0], [255]])\n",
        "\n",
        "    def predict(self):\n",
        "        self.u = np.round(dot(self.F, self.u))\n",
        "        self.P = dot(self.F, dot(self.P, self.F.T)) + self.Q\n",
        "        self.lastResult = self.u\n",
        "        return self.u\n",
        "\n",
        "    def correct(self, b, flag):\n",
        "        if not flag:\n",
        "            self.b = self.lastResult\n",
        "        else:\n",
        "            self.b = b\n",
        "        C = dot(self.A, dot(self.P, self.A.T))+self.R\n",
        "        K = dot(self.P, dot(self.A.T, np.linalg.inv(C)))\n",
        "        self.u = np.round(self.u + dot(K, (self.b - dot(self.A, self.u))))\n",
        "        self.P = self.P - dot(K, dot(C, K.T))\n",
        "        self.lastResult = self.u\n",
        "        return self.u"
      ],
      "metadata": {
        "id": "LTJbu-XWA4AJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNlZ0vgPCeS6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wLjuJfKfCeWn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tracker"
      ],
      "metadata": {
        "id": "gqP_WlTzB6N0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import python libraries\n",
        "import numpy as np\n",
        "#from KalmanFilter import KalmanFilterClass\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "\n",
        "class Track(object):\n",
        "\n",
        "\n",
        "    def __init__(self, prediction, trackIdCount):\n",
        "\n",
        "        self.track_id = trackIdCount  # identification of each track object\n",
        "        self.KF = KalmanFilterClass()  # KF instance to track this object\n",
        "        self.prediction = np.asarray(prediction)  # predicted centroids (x,y)\n",
        "        self.skipped_frames = 0  # number of frames skipped undetected\n",
        "        self.trace = []  # trace path\n",
        "\n",
        "\n",
        "class Tracker(object):\n",
        "\n",
        "\n",
        "    def __init__(self, dist_thresh, max_frames_to_skip, max_trace_length,\n",
        "                 trackIdCount):\n",
        "\n",
        "        self.dist_thresh = dist_thresh\n",
        "        self.max_frames_to_skip = max_frames_to_skip\n",
        "        self.max_trace_length = max_trace_length\n",
        "        self.tracks = []\n",
        "        self.trackIdCount = trackIdCount\n",
        "\n",
        "    def Update(self, detections):\n",
        "\n",
        "        # Create tracks if no tracks vector found\n",
        "        if (len(self.tracks) == 0):\n",
        "            for i in range(len(detections)):\n",
        "                track = Track(detections[i], self.trackIdCount)\n",
        "                self.trackIdCount += 1\n",
        "                self.tracks.append(track)\n",
        "\n",
        "        # Calculate cost using sum of square distance between\n",
        "        # predicted vs detected centroids\n",
        "        N = len(self.tracks)\n",
        "        M = len(detections)\n",
        "        cost = np.zeros(shape=(N, M))   # Cost matrix\n",
        "        for i in range(len(self.tracks)):\n",
        "            for j in range(len(detections)):\n",
        "                try:\n",
        "                    diff = self.tracks[i].prediction - detections[j]\n",
        "                    distance = np.sqrt(diff[0][0]*diff[0][0] +\n",
        "                                       diff[1][0]*diff[1][0])\n",
        "                    cost[i][j] = distance\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        # Let's average the squared ERROR\n",
        "        cost = (0.5) * cost\n",
        "        # Using Hungarian Algorithm assign the correct detected measurements\n",
        "        # to predicted tracks\n",
        "        assignment = []\n",
        "        for _ in range(N):\n",
        "            assignment.append(-1)\n",
        "        row_ind, col_ind = linear_sum_assignment(cost)\n",
        "        for i in range(len(row_ind)):\n",
        "            assignment[row_ind[i]] = col_ind[i]\n",
        "\n",
        "        # Identify tracks with no assignment, if any\n",
        "        un_assigned_tracks = []\n",
        "        for i in range(len(assignment)):\n",
        "            if (assignment[i] != -1):\n",
        "                # check for cost distance threshold.\n",
        "                # If cost is very high then un_assign (delete) the track\n",
        "                if (cost[i][assignment[i]] > self.dist_thresh):\n",
        "                    assignment[i] = -1\n",
        "                    un_assigned_tracks.append(i)\n",
        "                pass\n",
        "            else:\n",
        "                self.tracks[i].skipped_frames += 1\n",
        "\n",
        "        # If tracks are not detected for long time, remove them\n",
        "        del_tracks = []\n",
        "        for i in range(len(self.tracks)):\n",
        "            if (self.tracks[i].skipped_frames > self.max_frames_to_skip):\n",
        "                del_tracks.append(i)\n",
        "        if len(del_tracks) > 0:  # only when skipped frame exceeds max\n",
        "            for id in del_tracks:\n",
        "                if id < len(self.tracks):\n",
        "                    del self.tracks[id]\n",
        "                    del assignment[id]\n",
        "                else:\n",
        "                    print(\"ERROR: id is greater than length of tracks\")\n",
        "\n",
        "        # Now look for un_assigned detects\n",
        "        un_assigned_detects = []\n",
        "        for i in range(len(detections)):\n",
        "                if i not in assignment:\n",
        "                    un_assigned_detects.append(i)\n",
        "\n",
        "        # Start new tracks\n",
        "        if(len(un_assigned_detects) != 0):\n",
        "            for i in range(len(un_assigned_detects)):\n",
        "                track = Track(detections[un_assigned_detects[i]],\n",
        "                              self.trackIdCount)\n",
        "                self.trackIdCount += 1\n",
        "                self.tracks.append(track)\n",
        "\n",
        "        # Update KalmanFilter state, lastResults and tracks trace\n",
        "        for i in range(len(assignment)):\n",
        "            self.tracks[i].KF.predict()\n",
        "\n",
        "            if(assignment[i] != -1):\n",
        "                self.tracks[i].skipped_frames = 0\n",
        "                self.tracks[i].prediction = self.tracks[i].KF.correct(\n",
        "                                            detections[assignment[i]], 1)\n",
        "            else:\n",
        "                self.tracks[i].prediction = self.tracks[i].KF.correct(\n",
        "                                            np.array([[0], [0]]), 0)\n",
        "\n",
        "            if(len(self.tracks[i].trace) > self.max_trace_length):\n",
        "                for j in range(len(self.tracks[i].trace) -\n",
        "                               self.max_trace_length):\n",
        "                    del self.tracks[i].trace[j]\n",
        "\n",
        "            self.tracks[i].trace.append(self.tracks[i].prediction)\n",
        "            self.tracks[i].KF.lastResult = self.tracks[i].prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "8wPLq-x7B7T9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFnlan2yHulV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#YOLO"
      ],
      "metadata": {
        "id": "-02wxCIjHwwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##centroidtracker.py"
      ],
      "metadata": {
        "id": "qqzCo_E2IrOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance as dist\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "class CentroidTracker():\n",
        "    def __init__(self, maxDisappeared=50):\n",
        "        self.nextObjectID = 0\n",
        "        self.objects = OrderedDict()\n",
        "        self.disappeared = OrderedDict()\n",
        "        self.maxDisappeared = maxDisappeared\n",
        "    def register(self, centroid):\n",
        "        self.objects[self.nextObjectID]=centroid\n",
        "        self.disappeared[self.nextObjectID]=0\n",
        "        self.nextObjectID +=1\n",
        "    def deregister(self, objectID):\n",
        "        del self.objects[objectID]\n",
        "        del self.disappeared[objectID]\n",
        "    def update(self, rects):\n",
        "        if len(rects)==0:\n",
        "            for objectID in list(self.disappeared.keys()):\n",
        "                self.disappeared[objectID]+=1\n",
        "                if self.disappeared[objectID]>self.maxDisappeared:\n",
        "                    self.deregister(objectID)\n",
        "            return self.objects\n",
        "        inputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n",
        "        for (i, (xmin, ymin, xmax, ymax)) in enumerate(rects):\n",
        "            mid_x = int((xmin+xmax) / 2)\n",
        "            mid_y = int((ymin+ymax) / 2)\n",
        "            inputCentroids[i] = (mid_x, mid_y)\n",
        "        if len(self.objects)==0:\n",
        "            for i in range(0, len(inputCentroids)):\n",
        "                self.register(inputCentroids[i])\n",
        "        else:\n",
        "            objectIDs = list(self.objects.keys())\n",
        "            objectCentroids = list(self.objects.values())\n",
        "            D = dist.cdist(np.array(objectCentroids), inputCentroids)\n",
        "            rows = D.min(axis=1).argsort()\n",
        "            cols = D.argmin(axis=1)[rows]\n",
        "            usedRows = set()\n",
        "            usedCols = set()\n",
        "            for (row, col) in zip(rows, cols):\n",
        "                if row in usedRows or col in usedCols:\n",
        "                    continue\n",
        "                objectID = objectIDs[row]\n",
        "                self.objects[objectID] = inputCentroids[col]\n",
        "                self.disappeared[objectID] = 0\n",
        "                usedRows.add(row)\n",
        "                usedCols.add(col)\n",
        "                unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
        "                unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
        "                if D.shape[0] >= D.shape[1]:\n",
        "                    for row in unusedRows:\n",
        "                        objectID=objectIDs[row]\n",
        "                        self.disappeared[objectID] += 1\n",
        "                        if self.disappeared[objectID]> self.maxDisappeared:\n",
        "                            self.deregister(objectID)\n",
        "                        else:\n",
        "                            for col in unusedCols:\n",
        "                                self.register(inputCentroids[col])\n",
        "        return self.objects"
      ],
      "metadata": {
        "id": "urA8ys6TIrOt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##utils.py"
      ],
      "metadata": {
        "id": "bI5-kOt6IhR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Miscellaneous utility functions.\"\"\"\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n",
        "\n",
        "def compose(*funcs):\n",
        "    \"\"\"Compose arbitrarily many functions, evaluated left to right.\n",
        "\n",
        "    Reference: https://mathieularose.com/function-composition-in-python/\n",
        "    \"\"\"\n",
        "    # return lambda x: reduce(lambda v, f: f(v), funcs, x)\n",
        "    if funcs:\n",
        "        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n",
        "    else:\n",
        "        raise ValueError('Composition of empty sequence not supported.')\n",
        "\n",
        "def letterbox_image(image, size):\n",
        "    '''resize image with unchanged aspect ratio using padding'''\n",
        "    iw, ih = image.size\n",
        "    w, h = size\n",
        "    scale = min(w/iw, h/ih)\n",
        "    nw = int(iw*scale)\n",
        "    nh = int(ih*scale)\n",
        "\n",
        "    image = image.resize((nw,nh), Image.BICUBIC)\n",
        "    new_image = Image.new('RGB', size, (128,128,128))\n",
        "    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
        "    return new_image\n",
        "\n",
        "def rand(a=0, b=1):\n",
        "    return np.random.rand()*(b-a) + a\n",
        "\n",
        "def get_random_data(annotation_line, input_shape, random=True, max_boxes=20, jitter=.3, hue=.1, sat=1.5, val=1.5, proc_img=True):\n",
        "    '''random preprocessing for real-time data augmentation'''\n",
        "    line = annotation_line.split()\n",
        "    image = Image.open(line[0])\n",
        "    iw, ih = image.size\n",
        "    h, w = input_shape\n",
        "    box = np.array([np.array(list(map(int,box.split(',')))) for box in line[1:]])\n",
        "\n",
        "    if not random:\n",
        "        # resize image\n",
        "        scale = min(w/iw, h/ih)\n",
        "        nw = int(iw*scale)\n",
        "        nh = int(ih*scale)\n",
        "        dx = (w-nw)//2\n",
        "        dy = (h-nh)//2\n",
        "        image_data=0\n",
        "        if proc_img:\n",
        "            image = image.resize((nw,nh), Image.BICUBIC)\n",
        "            new_image = Image.new('RGB', (w,h), (128,128,128))\n",
        "            new_image.paste(image, (dx, dy))\n",
        "            image_data = np.array(new_image)/255.\n",
        "\n",
        "        # correct boxes\n",
        "        box_data = np.zeros((max_boxes,5))\n",
        "        if len(box)>0:\n",
        "            np.random.shuffle(box)\n",
        "            if len(box)>max_boxes: box = box[:max_boxes]\n",
        "            box[:, [0,2]] = box[:, [0,2]]*scale + dx\n",
        "            box[:, [1,3]] = box[:, [1,3]]*scale + dy\n",
        "            box_data[:len(box)] = box\n",
        "\n",
        "        return image_data, box_data\n",
        "\n",
        "    # resize image\n",
        "    new_ar = w/h * rand(1-jitter,1+jitter)/rand(1-jitter,1+jitter)\n",
        "    scale = rand(.25, 2)\n",
        "    if new_ar < 1:\n",
        "        nh = int(scale*h)\n",
        "        nw = int(nh*new_ar)\n",
        "    else:\n",
        "        nw = int(scale*w)\n",
        "        nh = int(nw/new_ar)\n",
        "    image = image.resize((nw,nh), Image.BICUBIC)\n",
        "\n",
        "    # place image\n",
        "    dx = int(rand(0, w-nw))\n",
        "    dy = int(rand(0, h-nh))\n",
        "    new_image = Image.new('RGB', (w,h), (128,128,128))\n",
        "    new_image.paste(image, (dx, dy))\n",
        "    image = new_image\n",
        "\n",
        "    # flip image or not\n",
        "    flip = rand()<.5\n",
        "    if flip: image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "    # distort image\n",
        "    hue = rand(-hue, hue)\n",
        "    sat = rand(1, sat) if rand()<.5 else 1/rand(1, sat)\n",
        "    val = rand(1, val) if rand()<.5 else 1/rand(1, val)\n",
        "    x = rgb_to_hsv(np.array(image)/255.)\n",
        "    x[..., 0] += hue\n",
        "    x[..., 0][x[..., 0]>1] -= 1\n",
        "    x[..., 0][x[..., 0]<0] += 1\n",
        "    x[..., 1] *= sat\n",
        "    x[..., 2] *= val\n",
        "    x[x>1] = 1\n",
        "    x[x<0] = 0\n",
        "    image_data = hsv_to_rgb(x) # numpy array, 0 to 1\n",
        "\n",
        "    # correct boxes\n",
        "    box_data = np.zeros((max_boxes,5))\n",
        "    if len(box)>0:\n",
        "        np.random.shuffle(box)\n",
        "        box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx\n",
        "        box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy\n",
        "        if flip: box[:, [0,2]] = w - box[:, [2,0]]\n",
        "        box[:, 0:2][box[:, 0:2]<0] = 0\n",
        "        box[:, 2][box[:, 2]>w] = w\n",
        "        box[:, 3][box[:, 3]>h] = h\n",
        "        box_w = box[:, 2] - box[:, 0]\n",
        "        box_h = box[:, 3] - box[:, 1]\n",
        "        box = box[np.logical_and(box_w>1, box_h>1)] # discard invalid box\n",
        "        if len(box)>max_boxes: box = box[:max_boxes]\n",
        "        box_data[:len(box)] = box\n",
        "\n",
        "    return image_data, box_data"
      ],
      "metadata": {
        "id": "fyMZ9J5yIhR1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##model compatibility"
      ],
      "metadata": {
        "id": "MkE7Q-b-HwD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y tensorflow keras jax jaxlib ml-dtypes\n",
        "#!pip install tensorflow==2.10.1 keras==2.10.0 ml-dtypes==0.5.0\n"
      ],
      "metadata": {
        "id": "GLQowaP1OxCk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "Ptw-ZXSqOxIk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip uninstall -y pillow\n",
        "!pip install pillow==10.2.0'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HUDfY5buRuZV",
        "outputId": "f42459eb-457f-47bd-c227-681f89db97f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!pip uninstall -y pillow\\n!pip install pillow==10.2.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from ultralytics import YOLO'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wmnj1fS5Q5jS",
        "outputId": "0f4a7bfd-7c0e-4f90-e316-45ccd39e1a78"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from ultralytics import YOLO'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#detector_car_person.py"
      ],
      "metadata": {
        "id": "5gtIloYGBu4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDkGko85LEXG",
        "outputId": "4b70bcc6-8ad2-45b6-9e79-a9c58ba6046d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.240-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cpu)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.240-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.240 ultralytics-thop-2.0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# detector_car_person_yolov8.py\n",
        "import cv2\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "#from tracker import Tracker  # Your existing Tracker class\n",
        "\n",
        "def resize(img, scale=200):\n",
        "    width = int(img.shape[1] * scale / 100)\n",
        "    height = int(img.shape[0] * scale / 100)\n",
        "    return cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def detect_video(video_path, model_path=\"yolov8n.pt\", output_path=\"\"):\n",
        "    model = YOLO(model_path)\n",
        "    tracker = Tracker(30, 0, 6, 0)  # Your tracker\n",
        "\n",
        "    vid = cv2.VideoCapture(video_path)\n",
        "    width, height = 400, 300\n",
        "    pause = False\n",
        "\n",
        "    if output_path:\n",
        "        fourcc = int(vid.get(cv2.CAP_PROP_FOURCC))\n",
        "        fps = vid.get(cv2.CAP_PROP_FPS)\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps,\n",
        "                              (int(0.78*width)-int(0.109*width), height-int(height*0.0608)))\n",
        "\n",
        "    track_colors = [(255,0,0), (0,255,0), (0,0,255), (255,255,0),\n",
        "                    (0,255,255), (255,0,255)]\n",
        "\n",
        "    while True:\n",
        "        ret, frame = vid.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        orig_frame = frame.copy()\n",
        "        frame = cv2.resize(frame, (width, height))\n",
        "        frame = frame[int(0.0608*height):height, int(0.109*width):int(0.78*width)]\n",
        "        h, w, _ = frame.shape\n",
        "\n",
        "        results = model.predict(frame, imgsz=416, conf=0.3, iou=0.4)[0]\n",
        "\n",
        "        boxes = results.boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2\n",
        "        class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
        "        scores = results.boxes.conf.cpu().numpy()\n",
        "\n",
        "        centroids = []\n",
        "        all_classes = []\n",
        "\n",
        "        for i, box in enumerate(boxes):\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "            mid_x, mid_y = int((x1+x2)/2), int((y1+y2)/2)\n",
        "            centroids.append(np.array([[mid_y],[mid_x]]))\n",
        "            predicted_class = model.names[class_ids[i]]\n",
        "            all_classes.append(predicted_class)\n",
        "\n",
        "            if predicted_class.lower() == \"person\":\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0,0,255), 1)\n",
        "                cv2.putText(frame, predicted_class, (x1, y1),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1)\n",
        "\n",
        "        # Update tracker\n",
        "        if centroids:\n",
        "            tracker.Update(centroids)\n",
        "            for t in tracker.tracks:\n",
        "                if len(t.trace) > 1:\n",
        "                    for j in range(len(t.trace)-1):\n",
        "                        y1, x1 = t.trace[j][0][0], t.trace[j][1][0]\n",
        "                        y2, x2 = t.trace[j+1][0][0], t.trace[j+1][1][0]\n",
        "                        clr = t.track_id\n",
        "                        cv2.arrowedLine(frame, (int(x1), int(y1)), (int(x2), int(y2)),\n",
        "                                        track_colors[clr % len(track_colors)],\n",
        "                                        line_type=cv2.LINE_AA, thickness=1)\n",
        "\n",
        "        cv2.putText(frame, f\"Count: {len(centroids)}\", (3,35),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,255,0), 1)\n",
        "\n",
        "        #cv2.imshow(\"result\", frame)\n",
        "        cv2_imshow(frame)\n",
        "        if output_path:\n",
        "            out.write(frame)\n",
        "\n",
        "        import time\n",
        "        time.sleep(0.03)  # ~30 FPS\n",
        "        '''key = cv2.waitKey(1) & 0xFF\n",
        "        if key == ord('q'):\n",
        "            break\n",
        "        if key == 112:  # 'p' pause\n",
        "            pause = not pause\n",
        "            while pause:\n",
        "                key2 = cv2.waitKey(30) & 0xFF\n",
        "                if key2 == 112:\n",
        "                    pause = False\n",
        "                    break'''\n",
        "\n",
        "    vid.release()\n",
        "    if output_path:\n",
        "        out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    '''\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    detect_video(\"test_video.mp4\", model_path=\"yolov8n.pt\", output_path=\"output.mp4\")'''\n"
      ],
      "metadata": {
        "id": "VAE6KsiwBxkC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#object_tracking.py"
      ],
      "metadata": {
        "id": "84PsgdxnBgGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n"
      ],
      "metadata": {
        "id": "74jXv3HoeL-v"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "def detect_video_inline(\n",
        "    video_path,\n",
        "    model_path=\"yolov8n.pt\",\n",
        "    output_video_path=\"output.mp4\",\n",
        "    output_frames_dir=\"/content/output_frames\",\n",
        "    save_every_n=1   # save every frame (increase to reduce storage)\n",
        "):\n",
        "    os.makedirs(output_frames_dir, exist_ok=True)\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps    = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_id = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        results = model.predict(frame, conf=0.3, iou=0.4, verbose=False)[0]\n",
        "        annotated = results.plot()\n",
        "\n",
        "        # Save video\n",
        "        out.write(annotated)\n",
        "\n",
        "        # Save frames\n",
        "        if frame_id % save_every_n == 0:\n",
        "            frame_path = os.path.join(\n",
        "                output_frames_dir, f\"frame_{frame_id:06d}.jpg\"\n",
        "            )\n",
        "            cv2.imwrite(frame_path, annotated)\n",
        "\n",
        "        frame_id += 1\n",
        "        if frame_id % 100 == 0:\n",
        "            print(f\"Processed {frame_id} frames\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(\"✅ Video saved to:\", output_video_path)\n",
        "    print(\"✅ Frames saved to:\", output_frames_dir)\n"
      ],
      "metadata": {
        "id": "QykOSwkUKlOq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detect_video_inline(\n",
        "    video_path=\"/content/drive/MyDrive/Jakir_Sir_office/WWD/test_videos/car.mp4\",\n",
        "    model_path=\"yolov8n.pt\",\n",
        "    output_video_path=\"/content/drive/MyDrive/Jakir_Sir_office/WWD/output/output_car.mp4\",\n",
        "    output_frames_dir=\"/content/drive/MyDrive/Jakir_Sir_office/WWD/output/frames\",\n",
        "    save_every_n=5   # save 1 frame every 5 frames\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdL4Js3AKlQ6",
        "outputId": "b42f08e6-e5b3-4252-8e68-9012cd2943a8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100 frames\n",
            "Processed 200 frames\n",
            "Processed 300 frames\n",
            "Processed 400 frames\n",
            "Processed 500 frames\n",
            "Processed 600 frames\n",
            "Processed 700 frames\n",
            "Processed 800 frames\n",
            "Processed 900 frames\n",
            "Processed 1000 frames\n",
            "Processed 1100 frames\n",
            "Processed 1200 frames\n",
            "Processed 1300 frames\n",
            "Processed 1400 frames\n",
            "Processed 1500 frames\n",
            "Processed 1600 frames\n",
            "Processed 1700 frames\n",
            "Processed 1800 frames\n",
            "Processed 1900 frames\n",
            "Processed 2000 frames\n",
            "Processed 2100 frames\n",
            "Processed 2200 frames\n",
            "Processed 2300 frames\n",
            "Processed 2400 frames\n",
            "Processed 2500 frames\n",
            "Processed 2600 frames\n",
            "Processed 2700 frames\n",
            "Processed 2800 frames\n",
            "Processed 2900 frames\n",
            "Processed 3000 frames\n",
            "Processed 3100 frames\n",
            "Processed 3200 frames\n",
            "Processed 3300 frames\n",
            "Processed 3400 frames\n",
            "Processed 3500 frames\n",
            "Processed 3600 frames\n",
            "Processed 3700 frames\n",
            "Processed 3800 frames\n",
            "Processed 3900 frames\n",
            "Processed 4000 frames\n",
            "Processed 4100 frames\n",
            "Processed 4200 frames\n",
            "Processed 4300 frames\n",
            "Processed 4400 frames\n",
            "Processed 4500 frames\n",
            "Processed 4600 frames\n",
            "Processed 4700 frames\n",
            "Processed 4800 frames\n",
            "Processed 4900 frames\n",
            "Processed 5000 frames\n",
            "Processed 5100 frames\n",
            "✅ Video saved to: /content/drive/MyDrive/Jakir_Sir_office/WWD/output/output_car.mp4\n",
            "✅ Frames saved to: /content/drive/MyDrive/Jakir_Sir_office/WWD/output/frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xlj22jKAKqDI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#then"
      ],
      "metadata": {
        "id": "8Jjdm3DaLhDe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q0xH61cgElT7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}