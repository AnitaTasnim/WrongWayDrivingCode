{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "c_7MudlXCfu9",
        "gqP_WlTzB6N0",
        "qqzCo_E2IrOs",
        "bI5-kOt6IhR0"
      ],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNCdk4/G63wNTX1eJuQvruX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnitaTasnim/WrongWayDrivingCode/blob/main/WrongWayDriving.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8-QJmFmcymgn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Start"
      ],
      "metadata": {
        "id": "TBn33SRX77-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Usmankhujaev/Wrong-direction-drivers-detection.git\n",
        "%cd Wrong-direction-drivers-detection\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXz2Rxov81ZK",
        "outputId": "abdedba8-6e70-49fc-9268-8295f730fb35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Wrong-direction-drivers-detection'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 88 (delta 12), reused 4 (delta 4), pack-reused 73 (from 1)\u001b[K\n",
            "Receiving objects: 100% (88/88), 1.53 MiB | 18.39 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n",
            "/content/Wrong-direction-drivers-detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlgXaw6gDBNs",
        "outputId": "0d742f6b-bbe6-4763-dbb2-e081e4402629"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detector_car_person.py\tmodel_data\t    requirenment.yml  train.py\n",
            "KalmanFilter.py\t\tobject_tracking.py  result\t      yolo3\n",
            "kmeans.py\t\tREADME.md\t    tracker.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights -O model_data/yolov3.weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzaksy7YLhoZ",
        "outputId": "146f26d9-1d1b-47cc-df62-b3771a206fa8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-17 17:00:13--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 172.67.185.199, 104.21.88.156, 2606:4700:3030::ac43:b9c7, ...\n",
            "Connecting to pjreddie.com (pjreddie.com)|172.67.185.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://data.pjreddie.com/files/yolov3.weights [following]\n",
            "--2025-12-17 17:00:13--  https://data.pjreddie.com/files/yolov3.weights\n",
            "Resolving data.pjreddie.com (data.pjreddie.com)... 104.21.88.156, 172.67.185.199, 2606:4700:3030::ac43:b9c7, ...\n",
            "Connecting to data.pjreddie.com (data.pjreddie.com)|104.21.88.156|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘model_data/yolov3.weights’\n",
            "\n",
            "model_data/yolov3.w 100%[===================>] 236.52M  41.2MB/s    in 5.3s    \n",
            "\n",
            "2025-12-17 17:00:19 (44.8 MB/s) - ‘model_data/yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls model_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUoGtnkMDBPR",
        "outputId": "c7d70358-0d81-43de-a2c3-ab805afea96a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car_class_2.txt  yolo_anchors.txt  yolov3.weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJzk6UZaDBTd",
        "outputId": "b2c74292-f1c8-4f54-f368-fb68de73bfa4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "algorithm2.png\n",
            "result.png\n",
            "wrong_direction_20191118_181432_2_01.jpg\n",
            "wrong_direction_20191118_181432_3_01.jpg\n",
            "wrong_direction_20191118_181433_3_00.jpg\n",
            "wrong_direction_20191118_181433_3_01.jpg\n",
            "wrong_direction_20191118_181501_2_01.jpg\n",
            "wrong_direction_20191118_181502_2_01.jpg\n",
            "wrong_direction_20191118_181502_3_00.jpg\n",
            "wrong_direction_20191118_181503_3_00.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JS6Kw7XDBVt",
        "outputId": "c8581ce0-6264-4ad5-ac17-9db14cfbccc1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CLEAN ENV FOR COLAB =====\n",
        "'''!pip uninstall -y numpy scipy\n",
        "!pip install --no-cache-dir numpy==1.26.4 scipy==1.11.4\n",
        "!pip install opencv-python filterpy matplotlib pillow ultralytics\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nDeia10Jqbj3",
        "outputId": "57bda267-3608-48fe-fd45-427964082879"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!pip uninstall -y numpy scipy\\n!pip install --no-cache-dir numpy==1.26.4 scipy==1.11.4\\n!pip install opencv-python filterpy matplotlib pillow ultralytics\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(np.char)\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "print(\"SciPy OK\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEWeD14bvYhQ",
        "outputId": "5175ac8d-6958-4573-cb14-ecb5c7847c1f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy: 2.0.2\n",
            "<module 'numpy.char' from '/usr/local/lib/python3.12/dist-packages/numpy/char/__init__.py'>\n",
            "SciPy OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip install -q --force-reinstall \\\n",
        "numpy==1.26.4 \\\n",
        "scipy==1.11.4 \\\n",
        "opencv-python \\\n",
        "opencv-contrib-python \\\n",
        "pillow \\\n",
        "matplotlib \\\n",
        "filterpy \\\n",
        "tensorflow==2.16.1 \\\n",
        "keras\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "e_vyZ2UAGDmh",
        "outputId": "e2b582cc-d50d-444f-b788-1ec69682eba5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!pip install -q --force-reinstall numpy==1.26.4 scipy==1.11.4 opencv-python opencv-contrib-python pillow matplotlib filterpy tensorflow==2.16.1 keras\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k6yZqMRIrnNw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#kalman_filter.py"
      ],
      "metadata": {
        "id": "c_7MudlXCfu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "\n",
        "class KalmanFilterClass(object):\n",
        "    def __init__(self):\n",
        "        self.dt = 5e-3\n",
        "        self.A = np.array([[1,0],[0,1]])\n",
        "        self.u = np.zeros((2,1))\n",
        "        self.b = np.array([[0],[255]])\n",
        "        self.P = np.diag((3.0, 3.0))\n",
        "        self.F = np.array([[1.0, self.dt], [0.0, 1.0]])\n",
        "        self.Q = np.eye(self.u.shape[0])\n",
        "        self.R = np.eye(self.b.shape[0])\n",
        "        self.lastResult = np.array([[0], [255]])\n",
        "\n",
        "    def predict(self):\n",
        "        self.u = np.round(dot(self.F, self.u))\n",
        "        self.P = dot(self.F, dot(self.P, self.F.T)) + self.Q\n",
        "        self.lastResult = self.u\n",
        "        return self.u\n",
        "\n",
        "    def correct(self, b, flag):\n",
        "        if not flag:\n",
        "            self.b = self.lastResult\n",
        "        else:\n",
        "            self.b = b\n",
        "        C = dot(self.A, dot(self.P, self.A.T))+self.R\n",
        "        K = dot(self.P, dot(self.A.T, np.linalg.inv(C)))\n",
        "        self.u = np.round(self.u + dot(K, (self.b - dot(self.A, self.u))))\n",
        "        self.P = self.P - dot(K, dot(C, K.T))\n",
        "        self.lastResult = self.u\n",
        "        return self.u"
      ],
      "metadata": {
        "id": "LTJbu-XWA4AJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNlZ0vgPCeS6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wLjuJfKfCeWn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tracker"
      ],
      "metadata": {
        "id": "gqP_WlTzB6N0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import python libraries\n",
        "import numpy as np\n",
        "#from KalmanFilter import KalmanFilterClass\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "\n",
        "class Track(object):\n",
        "\n",
        "\n",
        "    def __init__(self, prediction, trackIdCount):\n",
        "\n",
        "        self.track_id = trackIdCount  # identification of each track object\n",
        "        self.KF = KalmanFilterClass()  # KF instance to track this object\n",
        "        self.prediction = np.asarray(prediction)  # predicted centroids (x,y)\n",
        "        self.skipped_frames = 0  # number of frames skipped undetected\n",
        "        self.trace = []  # trace path\n",
        "\n",
        "\n",
        "class Tracker(object):\n",
        "\n",
        "\n",
        "    def __init__(self, dist_thresh, max_frames_to_skip, max_trace_length,\n",
        "                 trackIdCount):\n",
        "\n",
        "        self.dist_thresh = dist_thresh\n",
        "        self.max_frames_to_skip = max_frames_to_skip\n",
        "        self.max_trace_length = max_trace_length\n",
        "        self.tracks = []\n",
        "        self.trackIdCount = trackIdCount\n",
        "\n",
        "    def Update(self, detections):\n",
        "\n",
        "        # Create tracks if no tracks vector found\n",
        "        if (len(self.tracks) == 0):\n",
        "            for i in range(len(detections)):\n",
        "                track = Track(detections[i], self.trackIdCount)\n",
        "                self.trackIdCount += 1\n",
        "                self.tracks.append(track)\n",
        "\n",
        "        # Calculate cost using sum of square distance between\n",
        "        # predicted vs detected centroids\n",
        "        N = len(self.tracks)\n",
        "        M = len(detections)\n",
        "        cost = np.zeros(shape=(N, M))   # Cost matrix\n",
        "        for i in range(len(self.tracks)):\n",
        "            for j in range(len(detections)):\n",
        "                try:\n",
        "                    diff = self.tracks[i].prediction - detections[j]\n",
        "                    distance = np.sqrt(diff[0][0]*diff[0][0] +\n",
        "                                       diff[1][0]*diff[1][0])\n",
        "                    cost[i][j] = distance\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        # Let's average the squared ERROR\n",
        "        cost = (0.5) * cost\n",
        "        # Using Hungarian Algorithm assign the correct detected measurements\n",
        "        # to predicted tracks\n",
        "        assignment = []\n",
        "        for _ in range(N):\n",
        "            assignment.append(-1)\n",
        "        row_ind, col_ind = linear_sum_assignment(cost)\n",
        "        for i in range(len(row_ind)):\n",
        "            assignment[row_ind[i]] = col_ind[i]\n",
        "\n",
        "        # Identify tracks with no assignment, if any\n",
        "        un_assigned_tracks = []\n",
        "        for i in range(len(assignment)):\n",
        "            if (assignment[i] != -1):\n",
        "                # check for cost distance threshold.\n",
        "                # If cost is very high then un_assign (delete) the track\n",
        "                if (cost[i][assignment[i]] > self.dist_thresh):\n",
        "                    assignment[i] = -1\n",
        "                    un_assigned_tracks.append(i)\n",
        "                pass\n",
        "            else:\n",
        "                self.tracks[i].skipped_frames += 1\n",
        "\n",
        "        # If tracks are not detected for long time, remove them\n",
        "        del_tracks = []\n",
        "        for i in range(len(self.tracks)):\n",
        "            if (self.tracks[i].skipped_frames > self.max_frames_to_skip):\n",
        "                del_tracks.append(i)\n",
        "        if len(del_tracks) > 0:  # only when skipped frame exceeds max\n",
        "            for id in del_tracks:\n",
        "                if id < len(self.tracks):\n",
        "                    del self.tracks[id]\n",
        "                    del assignment[id]\n",
        "                else:\n",
        "                    print(\"ERROR: id is greater than length of tracks\")\n",
        "\n",
        "        # Now look for un_assigned detects\n",
        "        un_assigned_detects = []\n",
        "        for i in range(len(detections)):\n",
        "                if i not in assignment:\n",
        "                    un_assigned_detects.append(i)\n",
        "\n",
        "        # Start new tracks\n",
        "        if(len(un_assigned_detects) != 0):\n",
        "            for i in range(len(un_assigned_detects)):\n",
        "                track = Track(detections[un_assigned_detects[i]],\n",
        "                              self.trackIdCount)\n",
        "                self.trackIdCount += 1\n",
        "                self.tracks.append(track)\n",
        "\n",
        "        # Update KalmanFilter state, lastResults and tracks trace\n",
        "        for i in range(len(assignment)):\n",
        "            self.tracks[i].KF.predict()\n",
        "\n",
        "            if(assignment[i] != -1):\n",
        "                self.tracks[i].skipped_frames = 0\n",
        "                self.tracks[i].prediction = self.tracks[i].KF.correct(\n",
        "                                            detections[assignment[i]], 1)\n",
        "            else:\n",
        "                self.tracks[i].prediction = self.tracks[i].KF.correct(\n",
        "                                            np.array([[0], [0]]), 0)\n",
        "\n",
        "            if(len(self.tracks[i].trace) > self.max_trace_length):\n",
        "                for j in range(len(self.tracks[i].trace) -\n",
        "                               self.max_trace_length):\n",
        "                    del self.tracks[i].trace[j]\n",
        "\n",
        "            self.tracks[i].trace.append(self.tracks[i].prediction)\n",
        "            self.tracks[i].KF.lastResult = self.tracks[i].prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "8wPLq-x7B7T9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFnlan2yHulV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#YOLO"
      ],
      "metadata": {
        "id": "-02wxCIjHwwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##centroidtracker.py"
      ],
      "metadata": {
        "id": "qqzCo_E2IrOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance as dist\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "class CentroidTracker():\n",
        "    def __init__(self, maxDisappeared=50):\n",
        "        self.nextObjectID = 0\n",
        "        self.objects = OrderedDict()\n",
        "        self.disappeared = OrderedDict()\n",
        "        self.maxDisappeared = maxDisappeared\n",
        "    def register(self, centroid):\n",
        "        self.objects[self.nextObjectID]=centroid\n",
        "        self.disappeared[self.nextObjectID]=0\n",
        "        self.nextObjectID +=1\n",
        "    def deregister(self, objectID):\n",
        "        del self.objects[objectID]\n",
        "        del self.disappeared[objectID]\n",
        "    def update(self, rects):\n",
        "        if len(rects)==0:\n",
        "            for objectID in list(self.disappeared.keys()):\n",
        "                self.disappeared[objectID]+=1\n",
        "                if self.disappeared[objectID]>self.maxDisappeared:\n",
        "                    self.deregister(objectID)\n",
        "            return self.objects\n",
        "        inputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n",
        "        for (i, (xmin, ymin, xmax, ymax)) in enumerate(rects):\n",
        "            mid_x = int((xmin+xmax) / 2)\n",
        "            mid_y = int((ymin+ymax) / 2)\n",
        "            inputCentroids[i] = (mid_x, mid_y)\n",
        "        if len(self.objects)==0:\n",
        "            for i in range(0, len(inputCentroids)):\n",
        "                self.register(inputCentroids[i])\n",
        "        else:\n",
        "            objectIDs = list(self.objects.keys())\n",
        "            objectCentroids = list(self.objects.values())\n",
        "            D = dist.cdist(np.array(objectCentroids), inputCentroids)\n",
        "            rows = D.min(axis=1).argsort()\n",
        "            cols = D.argmin(axis=1)[rows]\n",
        "            usedRows = set()\n",
        "            usedCols = set()\n",
        "            for (row, col) in zip(rows, cols):\n",
        "                if row in usedRows or col in usedCols:\n",
        "                    continue\n",
        "                objectID = objectIDs[row]\n",
        "                self.objects[objectID] = inputCentroids[col]\n",
        "                self.disappeared[objectID] = 0\n",
        "                usedRows.add(row)\n",
        "                usedCols.add(col)\n",
        "                unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
        "                unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
        "                if D.shape[0] >= D.shape[1]:\n",
        "                    for row in unusedRows:\n",
        "                        objectID=objectIDs[row]\n",
        "                        self.disappeared[objectID] += 1\n",
        "                        if self.disappeared[objectID]> self.maxDisappeared:\n",
        "                            self.deregister(objectID)\n",
        "                        else:\n",
        "                            for col in unusedCols:\n",
        "                                self.register(inputCentroids[col])\n",
        "        return self.objects"
      ],
      "metadata": {
        "id": "urA8ys6TIrOt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##utils.py"
      ],
      "metadata": {
        "id": "bI5-kOt6IhR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Miscellaneous utility functions.\"\"\"\n",
        "\n",
        "from functools import reduce\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n",
        "\n",
        "def compose(*funcs):\n",
        "    \"\"\"Compose arbitrarily many functions, evaluated left to right.\n",
        "\n",
        "    Reference: https://mathieularose.com/function-composition-in-python/\n",
        "    \"\"\"\n",
        "    # return lambda x: reduce(lambda v, f: f(v), funcs, x)\n",
        "    if funcs:\n",
        "        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n",
        "    else:\n",
        "        raise ValueError('Composition of empty sequence not supported.')\n",
        "\n",
        "def letterbox_image(image, size):\n",
        "    '''resize image with unchanged aspect ratio using padding'''\n",
        "    iw, ih = image.size\n",
        "    w, h = size\n",
        "    scale = min(w/iw, h/ih)\n",
        "    nw = int(iw*scale)\n",
        "    nh = int(ih*scale)\n",
        "\n",
        "    image = image.resize((nw,nh), Image.BICUBIC)\n",
        "    new_image = Image.new('RGB', size, (128,128,128))\n",
        "    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
        "    return new_image\n",
        "\n",
        "def rand(a=0, b=1):\n",
        "    return np.random.rand()*(b-a) + a\n",
        "\n",
        "def get_random_data(annotation_line, input_shape, random=True, max_boxes=20, jitter=.3, hue=.1, sat=1.5, val=1.5, proc_img=True):\n",
        "    '''random preprocessing for real-time data augmentation'''\n",
        "    line = annotation_line.split()\n",
        "    image = Image.open(line[0])\n",
        "    iw, ih = image.size\n",
        "    h, w = input_shape\n",
        "    box = np.array([np.array(list(map(int,box.split(',')))) for box in line[1:]])\n",
        "\n",
        "    if not random:\n",
        "        # resize image\n",
        "        scale = min(w/iw, h/ih)\n",
        "        nw = int(iw*scale)\n",
        "        nh = int(ih*scale)\n",
        "        dx = (w-nw)//2\n",
        "        dy = (h-nh)//2\n",
        "        image_data=0\n",
        "        if proc_img:\n",
        "            image = image.resize((nw,nh), Image.BICUBIC)\n",
        "            new_image = Image.new('RGB', (w,h), (128,128,128))\n",
        "            new_image.paste(image, (dx, dy))\n",
        "            image_data = np.array(new_image)/255.\n",
        "\n",
        "        # correct boxes\n",
        "        box_data = np.zeros((max_boxes,5))\n",
        "        if len(box)>0:\n",
        "            np.random.shuffle(box)\n",
        "            if len(box)>max_boxes: box = box[:max_boxes]\n",
        "            box[:, [0,2]] = box[:, [0,2]]*scale + dx\n",
        "            box[:, [1,3]] = box[:, [1,3]]*scale + dy\n",
        "            box_data[:len(box)] = box\n",
        "\n",
        "        return image_data, box_data\n",
        "\n",
        "    # resize image\n",
        "    new_ar = w/h * rand(1-jitter,1+jitter)/rand(1-jitter,1+jitter)\n",
        "    scale = rand(.25, 2)\n",
        "    if new_ar < 1:\n",
        "        nh = int(scale*h)\n",
        "        nw = int(nh*new_ar)\n",
        "    else:\n",
        "        nw = int(scale*w)\n",
        "        nh = int(nw/new_ar)\n",
        "    image = image.resize((nw,nh), Image.BICUBIC)\n",
        "\n",
        "    # place image\n",
        "    dx = int(rand(0, w-nw))\n",
        "    dy = int(rand(0, h-nh))\n",
        "    new_image = Image.new('RGB', (w,h), (128,128,128))\n",
        "    new_image.paste(image, (dx, dy))\n",
        "    image = new_image\n",
        "\n",
        "    # flip image or not\n",
        "    flip = rand()<.5\n",
        "    if flip: image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "    # distort image\n",
        "    hue = rand(-hue, hue)\n",
        "    sat = rand(1, sat) if rand()<.5 else 1/rand(1, sat)\n",
        "    val = rand(1, val) if rand()<.5 else 1/rand(1, val)\n",
        "    x = rgb_to_hsv(np.array(image)/255.)\n",
        "    x[..., 0] += hue\n",
        "    x[..., 0][x[..., 0]>1] -= 1\n",
        "    x[..., 0][x[..., 0]<0] += 1\n",
        "    x[..., 1] *= sat\n",
        "    x[..., 2] *= val\n",
        "    x[x>1] = 1\n",
        "    x[x<0] = 0\n",
        "    image_data = hsv_to_rgb(x) # numpy array, 0 to 1\n",
        "\n",
        "    # correct boxes\n",
        "    box_data = np.zeros((max_boxes,5))\n",
        "    if len(box)>0:\n",
        "        np.random.shuffle(box)\n",
        "        box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx\n",
        "        box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy\n",
        "        if flip: box[:, [0,2]] = w - box[:, [2,0]]\n",
        "        box[:, 0:2][box[:, 0:2]<0] = 0\n",
        "        box[:, 2][box[:, 2]>w] = w\n",
        "        box[:, 3][box[:, 3]>h] = h\n",
        "        box_w = box[:, 2] - box[:, 0]\n",
        "        box_h = box[:, 3] - box[:, 1]\n",
        "        box = box[np.logical_and(box_w>1, box_h>1)] # discard invalid box\n",
        "        if len(box)>max_boxes: box = box[:max_boxes]\n",
        "        box_data[:len(box)] = box\n",
        "\n",
        "    return image_data, box_data"
      ],
      "metadata": {
        "id": "fyMZ9J5yIhR1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##model compatibility"
      ],
      "metadata": {
        "id": "MkE7Q-b-HwD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y tensorflow keras jax jaxlib ml-dtypes\n",
        "#!pip install tensorflow==2.10.1 keras==2.10.0 ml-dtypes==0.5.0\n"
      ],
      "metadata": {
        "id": "GLQowaP1OxCk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "Ptw-ZXSqOxIk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''!pip uninstall -y pillow\n",
        "!pip install pillow==10.2.0'''"
      ],
      "metadata": {
        "id": "HUDfY5buRuZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from ultralytics import YOLO'''"
      ],
      "metadata": {
        "id": "wmnj1fS5Q5jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#detector_car_person.py"
      ],
      "metadata": {
        "id": "5gtIloYGBu4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# detector_car_person_yolov8.py\n",
        "import cv2\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "#from tracker import Tracker  # Your existing Tracker class\n",
        "\n",
        "def resize(img, scale=200):\n",
        "    width = int(img.shape[1] * scale / 100)\n",
        "    height = int(img.shape[0] * scale / 100)\n",
        "    return cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def detect_video(video_path, model_path=\"yolov8n.pt\", output_path=\"\"):\n",
        "    model = YOLO(model_path)\n",
        "    tracker = Tracker(30, 0, 6, 0)  # Your tracker\n",
        "\n",
        "    vid = cv2.VideoCapture(video_path)\n",
        "    width, height = 400, 300\n",
        "    pause = False\n",
        "\n",
        "    if output_path:\n",
        "        fourcc = int(vid.get(cv2.CAP_PROP_FOURCC))\n",
        "        fps = vid.get(cv2.CAP_PROP_FPS)\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps,\n",
        "                              (int(0.78*width)-int(0.109*width), height-int(height*0.0608)))\n",
        "\n",
        "    track_colors = [(255,0,0), (0,255,0), (0,0,255), (255,255,0),\n",
        "                    (0,255,255), (255,0,255)]\n",
        "\n",
        "    while True:\n",
        "        ret, frame = vid.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        orig_frame = frame.copy()\n",
        "        frame = cv2.resize(frame, (width, height))\n",
        "        frame = frame[int(0.0608*height):height, int(0.109*width):int(0.78*width)]\n",
        "        h, w, _ = frame.shape\n",
        "\n",
        "        results = model.predict(frame, imgsz=416, conf=0.3, iou=0.4)[0]\n",
        "\n",
        "        boxes = results.boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2\n",
        "        class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
        "        scores = results.boxes.conf.cpu().numpy()\n",
        "\n",
        "        centroids = []\n",
        "        all_classes = []\n",
        "\n",
        "        for i, box in enumerate(boxes):\n",
        "            x1, y1, x2, y2 = box.astype(int)\n",
        "            mid_x, mid_y = int((x1+x2)/2), int((y1+y2)/2)\n",
        "            centroids.append(np.array([[mid_y],[mid_x]]))\n",
        "            predicted_class = model.names[class_ids[i]]\n",
        "            all_classes.append(predicted_class)\n",
        "\n",
        "            if predicted_class.lower() == \"person\":\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0,0,255), 1)\n",
        "                cv2.putText(frame, predicted_class, (x1, y1),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1)\n",
        "\n",
        "        # Update tracker\n",
        "        if centroids:\n",
        "            tracker.Update(centroids)\n",
        "            for t in tracker.tracks:\n",
        "                if len(t.trace) > 1:\n",
        "                    for j in range(len(t.trace)-1):\n",
        "                        y1, x1 = t.trace[j][0][0], t.trace[j][1][0]\n",
        "                        y2, x2 = t.trace[j+1][0][0], t.trace[j+1][1][0]\n",
        "                        clr = t.track_id\n",
        "                        cv2.arrowedLine(frame, (int(x1), int(y1)), (int(x2), int(y2)),\n",
        "                                        track_colors[clr % len(track_colors)],\n",
        "                                        line_type=cv2.LINE_AA, thickness=1)\n",
        "\n",
        "        cv2.putText(frame, f\"Count: {len(centroids)}\", (3,35),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,255,0), 1)\n",
        "\n",
        "        #cv2.imshow(\"result\", frame)\n",
        "        cv2_imshow(frame)\n",
        "        if output_path:\n",
        "            out.write(frame)\n",
        "\n",
        "        import time\n",
        "        time.sleep(0.03)  # ~30 FPS\n",
        "        '''key = cv2.waitKey(1) & 0xFF\n",
        "        if key == ord('q'):\n",
        "            break\n",
        "        if key == 112:  # 'p' pause\n",
        "            pause = not pause\n",
        "            while pause:\n",
        "                key2 = cv2.waitKey(30) & 0xFF\n",
        "                if key2 == 112:\n",
        "                    pause = False\n",
        "                    break'''\n",
        "\n",
        "    vid.release()\n",
        "    if output_path:\n",
        "        out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    detect_video(\"test_video.mp4\", model_path=\"yolov8n.pt\", output_path=\"output.mp4\")\n"
      ],
      "metadata": {
        "id": "VAE6KsiwBxkC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#object_tracking.py"
      ],
      "metadata": {
        "id": "84PsgdxnBgGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n"
      ],
      "metadata": {
        "id": "74jXv3HoeL-v"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# object_tracking_inline.py\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "def detect_image_inline(model_path=\"yolov8n.pt\", img_path=None):\n",
        "    \"\"\"\n",
        "    Detect objects in a single image using YOLOv8\n",
        "    \"\"\"\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    if img_path is None:\n",
        "        img_path = input(\"Input image filename: \")\n",
        "\n",
        "    image = np.array(Image.open(img_path))\n",
        "    results = model.predict(image, imgsz=416, conf=0.3, iou=0.4)[0]\n",
        "\n",
        "    boxes = results.boxes.xyxy.cpu().numpy()\n",
        "    class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
        "    scores = results.boxes.conf.cpu().numpy()\n",
        "    names = model.names\n",
        "\n",
        "    print(\"\\nDetected objects:\")\n",
        "    for i, box in enumerate(boxes):\n",
        "        print(f\"{names[class_ids[i]]} - confidence {scores[i]:.2f} - box {box}\")\n",
        "\n",
        "    # Show annotated image\n",
        "    annotated_frame = results.plot()\n",
        "    cv2.imshow(\"YOLOv8 Detection\", annotated_frame)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "def detect_video_inline(video_path, model_path=\"yolov8n.pt\", output_path=\"output.mp4\"):\n",
        "    \"\"\"\n",
        "    Detect objects in a video using YOLOv8\n",
        "    \"\"\"\n",
        "    #from detector_car_person_yolov8 import detect_video\n",
        "    detect_video(video_path, model_path=model_path, output_path=output_path)\n"
      ],
      "metadata": {
        "id": "bJ9OW3f5A4Fg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Inline image detection\n",
        "detect_image_inline(model_path=\"yolov8n.pt\", img_path=\"/content/my_image.jpg\")\n"
      ],
      "metadata": {
        "id": "GhNKk3tFBkSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Inline video detection\n",
        "detect_video_inline(video_path=\"/content/drive/MyDrive/Jakir_Sir_office/WWD/test_videos/car.mp4\",\n",
        "                    model_path=\"yolov8n.pt\",\n",
        "                    output_path=\"/content/drive/MyDrive/Jakir_Sir_office/WWD/output/output_car.mp4\")\n"
      ],
      "metadata": {
        "id": "pu9aGlm7BkUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#next"
      ],
      "metadata": {
        "id": "QNWY4qsHBkqB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q0xH61cgElT7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}